{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pipelines",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8YHn-kG2p_x"
      },
      "source": [
        "<h1>Cascade Cup 2020 submission for round 2 (datathon)</h1>\r\n",
        "\r\n",
        "Team Name: **ML Hacksters**\r\n",
        "\r\n",
        "Member 1: **Nippun Sharma (inbox.nippun@gmail.com)**\r\n",
        "\r\n",
        "Member 2: **Naveen Saisreenivas Thota (naveensaisreenivas@gmail.com)**\r\n",
        "\r\n",
        "Public Leaderboard Score: *80.89416646619021*\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mZYGBvvA8qgk"
      },
      "source": [
        "**Preprocessing Techniques**:\r\n",
        "\r\n",
        "\r\n",
        "1.   `RobustScaler` (`sklearn.preprocessing`)\r\n",
        "2.   `StandardScaler` (`sklearn.preprocessing`)\r\n",
        "\r\n",
        "**Feature engineering**:\r\n",
        "\r\n",
        "\r\n",
        "1.   `PolynomialFeatures` (`sklearn.preprocessing`)\r\n",
        "2.   `FeatureAgglomeration` (`sklearn.cluster`)\r\n",
        "\r\n",
        "\r\n",
        "**Models Used**: \r\n",
        "\r\n",
        "1.   `HistGradientBoostingClassifier` (`sklearn.ensemble`)\r\n",
        "2.   `AdaBoostClassifier` (`sklearn.ensemble`)\r\n",
        "\r\n",
        "> For ease of underastanding and keeping the notebook clean we have used    `sklearn.pipeline.Pipeline`\r\n",
        "\r\n",
        "\r\n",
        "> For making the model more general we are using `Out of Fold` prediction technique (num folds = 5). \r\n",
        "\r\n",
        "\r\n",
        "> To combine the predictions for each pipeline we are using the mean of each component of the 5 which are trained on 5 different folds of data.\r\n",
        "\r\n",
        "> Finally, we are taking the weighted mean (weights: 0.9 and 0.1 respectively) of both the pipes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kvYaSCxHalq"
      },
      "source": [
        "### EDA"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtXqCoV2ASt8"
      },
      "source": [
        "# Importing necessary functions and libraries\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.experimental import enable_hist_gradient_boosting\n",
        "from sklearn.ensemble import HistGradientBoostingClassifier, AdaBoostClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.preprocessing import RobustScaler, StandardScaler, PolynomialFeatures\n",
        "from sklearn.cluster import FeatureAgglomeration\n",
        "\n",
        "# creating framework of pipeline 1\n",
        "pipe_1_frame = Pipeline([('rs', RobustScaler(quantile_range=(0.2282, 0.7468))),\n",
        "                   ('adaboost', AdaBoostClassifier(\n",
        "                                                   learning_rate=0.228022,\n",
        "                                                   base_estimator=DecisionTreeClassifier(max_depth=6),\n",
        "                                                   n_estimators=284,\n",
        "                                                   random_state=2019))\n",
        "                   ])\n",
        "\n",
        "# creating framework of pipeline 2\n",
        "pipe_2_frame = Pipeline([('ss', StandardScaler()),\n",
        "                   ('pf', PolynomialFeatures(degree=2)),\n",
        "                   ('fa', FeatureAgglomeration(affinity='manhattan',\n",
        "                                               linkage='complete',\n",
        "                                               n_clusters=78,\n",
        "                                               pooling_func=np.mean)),\n",
        "                   ('histgradientboost', HistGradientBoostingClassifier(\n",
        "                                                                        l2_regularization=0.26386,\n",
        "                                                                        learning_rate=0.0223608,\n",
        "                                                                        max_leaf_nodes=171,\n",
        "                                                                        min_samples_leaf=6,\n",
        "                                                                        n_iter_no_change=19,\n",
        "                                                                        validation_fraction=0.016209,\n",
        "                                                                        random_state=2019))\n",
        "                   ])"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1sZPHpAfDovx"
      },
      "source": [
        "# change the path of files accordingly.\n",
        "\n",
        "train_data = pd.read_csv(\"train_age_dataset.csv\")\n",
        "test_data = pd.read_csv(\"test_age_dataset.csv\")\n",
        "sample = pd.read_csv(\"sample_submission.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIRf_G4aD2tp"
      },
      "source": [
        "# separating dependent and independent variables for train data\n",
        "x_train = train_data.drop(\"age_group\", 1).values\n",
        "y_train = train_data[\"age_group\"].values\n",
        "\n",
        "# test data\n",
        "x_test = test_data.values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aVuy-IceB1M7"
      },
      "source": [
        "# Importing necessary functions and libraries\r\n",
        "\r\n",
        "from sklearn.model_selection import KFold # for out of fold prediction\r\n",
        "from sklearn.base import clone # for creating copies\r\n",
        "import joblib # for saving trained pipes\r\n",
        "import os\r\n",
        "\r\n",
        "# creating directories to save trained pipelines.\r\n",
        "dir_1 = \"pipe_1/\"\r\n",
        "dir_2 = \"pipe_2/\"\r\n",
        "\r\n",
        "os.mkdir(dir_1)\r\n",
        "os.mkdir(dir_2)\r\n",
        "\r\n",
        "NUM_FOLDS = 5\r\n",
        "kf = KFold(n_splits=NUM_FOLDS)\r\n",
        "\r\n",
        "# training and saving pipelines for each fold.\r\n",
        "for idx, (train_idx, test_idx) in enumerate(kf.split(x_train, y_train)):\r\n",
        "  print(\"Training on fold {}\".format(idx + 1))\r\n",
        "\r\n",
        "  temp_1 = clone(pipe_1_frame)\r\n",
        "  temp_2 = clone(pipe_2_frame)\r\n",
        "\r\n",
        "  temp_1.fit(x_train[train_idx, :], y_train[train_idx])\r\n",
        "  print(\"Pipe 1 trained...\")\r\n",
        "  temp_2.fit(x_train[train_idx, :], y_train[train_idx])\r\n",
        "  print(\"Pipe 2 trained...\")\r\n",
        "\r\n",
        "  # storing the pipeline on disk.\r\n",
        "  joblib.dump(temp_1, \"pipe_1/\" + str(idx+1) + \".pkl\")\r\n",
        "  joblib.dump(temp_2, \"pipe_2/\" + str(idx+1) + \".pkl\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hi1tiQpzmfer"
      },
      "source": [
        "# making predictions on test data.\r\n",
        "\r\n",
        "# lists for storing probabilities on test data for each fold.\r\n",
        "preds_1 = []\r\n",
        "preds_2 = []\r\n",
        "\r\n",
        "for idx in range(1,6):\r\n",
        "  # loading the pipelines from disk.\r\n",
        "  temp_1 = joblib.load(dir_1 + str(idx) + \".pkl\")\r\n",
        "  temp_2 = joblib.load(dir_2 + str(idx) + \".pkl\")\r\n",
        "\r\n",
        "  preds_1.append(temp_1.predict_proba(x_test))\r\n",
        "  preds_2.append(temp_2.predict_proba(x_test))\r\n",
        "\r\n",
        "# converting to numpy arrays\r\n",
        "preds_1 = np.array(preds_1)\r\n",
        "preds_2 = np.array(preds_2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m__SFKeXn1m7"
      },
      "source": [
        "# combining probabilities of each fold by taking mean of all.\r\n",
        "preds_1 = np.mean(preds_1, axis=0)\r\n",
        "preds_2 = np.mean(preds_2, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tDK3LHsEMZw"
      },
      "source": [
        "# weigthed mean of probabilites from the two pipeline frameworks.\n",
        "preds = preds_1 * 0.9 + preds_2 * 0.1\n",
        "\n",
        "# getting classes from final probabilites (hard classification from soft classification). \n",
        "preds = np.argmax(preds, axis=1) + 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPm6hlCtFCeb"
      },
      "source": [
        "# creating submission file\n",
        "sample['prediction'] = np.ravel(preds)\n",
        "sample.to_csv(\"final_submission.csv\", index=False)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}